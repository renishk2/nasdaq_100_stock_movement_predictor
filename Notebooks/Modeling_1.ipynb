{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary packages \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset \n",
    "\n",
    "apple_df = pd.read_csv('../clean_apple_stocks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Closing Yday</th>\n",
       "      <th>Adj Closing Price Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>7.664286</td>\n",
       "      <td>7.699643</td>\n",
       "      <td>7.616071</td>\n",
       "      <td>7.656429</td>\n",
       "      <td>6.573935</td>\n",
       "      <td>601904800</td>\n",
       "      <td>6.562591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>7.656429</td>\n",
       "      <td>7.686786</td>\n",
       "      <td>7.526786</td>\n",
       "      <td>7.534643</td>\n",
       "      <td>6.469369</td>\n",
       "      <td>552160000</td>\n",
       "      <td>6.573935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>7.562500</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>7.466071</td>\n",
       "      <td>7.520714</td>\n",
       "      <td>6.457407</td>\n",
       "      <td>477131200</td>\n",
       "      <td>6.469369</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>7.510714</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>7.466429</td>\n",
       "      <td>7.570714</td>\n",
       "      <td>6.500339</td>\n",
       "      <td>447610800</td>\n",
       "      <td>6.457407</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>7.607143</td>\n",
       "      <td>7.444643</td>\n",
       "      <td>7.503929</td>\n",
       "      <td>6.442997</td>\n",
       "      <td>462229600</td>\n",
       "      <td>6.500339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Open      High       Low     Close  Adj Close     Volume  \\\n",
       "0  2010-01-05  7.664286  7.699643  7.616071  7.656429   6.573935  601904800   \n",
       "1  2010-01-06  7.656429  7.686786  7.526786  7.534643   6.469369  552160000   \n",
       "2  2010-01-07  7.562500  7.571429  7.466071  7.520714   6.457407  477131200   \n",
       "3  2010-01-08  7.510714  7.571429  7.466429  7.570714   6.500339  447610800   \n",
       "4  2010-01-11  7.600000  7.607143  7.444643  7.503929   6.442997  462229600   \n",
       "\n",
       "   Adj Closing Yday  Adj Closing Price Change  \n",
       "0          6.562591                         1  \n",
       "1          6.573935                         0  \n",
       "2          6.469369                         0  \n",
       "3          6.457407                         1  \n",
       "4          6.500339                         0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2942, 9)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Logistic Regression \n",
    "\n",
    "# Assigning Variables \n",
    "\n",
    "X = apple_df.drop(['Adj Closing Price Change', 'Date'], axis= 1)\n",
    "y = apple_df['Adj Closing Price Change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test \n",
    "\n",
    "# Import the package \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the test and remainder\n",
    "\n",
    "X_rem, X_test, y_rem, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "\n",
    "# Splitting the train and validation \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rem, y_rem, test_size=0.2, stratify=y_rem, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the shape of our train/test split \n",
    "\n",
    "#print(f\"The shape of our train set is {}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data \n",
    "\n",
    "# Import the package \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.6642380085003036\n",
      "Validation score 0.6359223300970874\n",
      "Test score 0.6636466591166478\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "#Importing the package \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Intializing the log reg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fitting our training data\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Printing results\n",
    "print(\"Train score\", logreg.score(X_train_scaled, y_train))\n",
    "print(\"Validation score\", logreg.score(X_val_scaled, y_val))\n",
    "print(\"Test score\", logreg.score(X_test_scaled, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on EDA 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our clean dataset from the EDA - 2 Notebook \n",
    "\n",
    "stocks_df = pd.read_csv('../clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Closing Yday</th>\n",
       "      <th>Adj Closing Price Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.664286</td>\n",
       "      <td>7.699643</td>\n",
       "      <td>7.616071</td>\n",
       "      <td>7.656429</td>\n",
       "      <td>6.573935</td>\n",
       "      <td>601904800</td>\n",
       "      <td>6.562591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.656429</td>\n",
       "      <td>7.686786</td>\n",
       "      <td>7.526786</td>\n",
       "      <td>7.534643</td>\n",
       "      <td>6.469369</td>\n",
       "      <td>552160000</td>\n",
       "      <td>6.573935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.562500</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>7.466071</td>\n",
       "      <td>7.520714</td>\n",
       "      <td>6.457407</td>\n",
       "      <td>477131200</td>\n",
       "      <td>6.469369</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.510714</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>7.466429</td>\n",
       "      <td>7.570714</td>\n",
       "      <td>6.500339</td>\n",
       "      <td>447610800</td>\n",
       "      <td>6.457407</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.600000</td>\n",
       "      <td>7.607143</td>\n",
       "      <td>7.444643</td>\n",
       "      <td>7.503929</td>\n",
       "      <td>6.442997</td>\n",
       "      <td>462229600</td>\n",
       "      <td>6.500339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.471071</td>\n",
       "      <td>7.491786</td>\n",
       "      <td>7.372143</td>\n",
       "      <td>7.418571</td>\n",
       "      <td>6.369709</td>\n",
       "      <td>594459600</td>\n",
       "      <td>6.442997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.423929</td>\n",
       "      <td>7.533214</td>\n",
       "      <td>7.289286</td>\n",
       "      <td>7.523214</td>\n",
       "      <td>6.459555</td>\n",
       "      <td>605892000</td>\n",
       "      <td>6.369709</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.503929</td>\n",
       "      <td>7.516429</td>\n",
       "      <td>7.465000</td>\n",
       "      <td>7.479643</td>\n",
       "      <td>6.422143</td>\n",
       "      <td>432894000</td>\n",
       "      <td>6.459555</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.533214</td>\n",
       "      <td>7.557143</td>\n",
       "      <td>7.352500</td>\n",
       "      <td>7.354643</td>\n",
       "      <td>6.314816</td>\n",
       "      <td>594067600</td>\n",
       "      <td>6.422143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.440357</td>\n",
       "      <td>7.685357</td>\n",
       "      <td>7.401429</td>\n",
       "      <td>7.680000</td>\n",
       "      <td>6.594175</td>\n",
       "      <td>730007600</td>\n",
       "      <td>6.314816</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open      High       Low     Close  Adj Close     Volume  \\\n",
       "0  7.664286  7.699643  7.616071  7.656429   6.573935  601904800   \n",
       "1  7.656429  7.686786  7.526786  7.534643   6.469369  552160000   \n",
       "2  7.562500  7.571429  7.466071  7.520714   6.457407  477131200   \n",
       "3  7.510714  7.571429  7.466429  7.570714   6.500339  447610800   \n",
       "4  7.600000  7.607143  7.444643  7.503929   6.442997  462229600   \n",
       "5  7.471071  7.491786  7.372143  7.418571   6.369709  594459600   \n",
       "6  7.423929  7.533214  7.289286  7.523214   6.459555  605892000   \n",
       "7  7.503929  7.516429  7.465000  7.479643   6.422143  432894000   \n",
       "8  7.533214  7.557143  7.352500  7.354643   6.314816  594067600   \n",
       "9  7.440357  7.685357  7.401429  7.680000   6.594175  730007600   \n",
       "\n",
       "   Adj Closing Yday  Adj Closing Price Change  \n",
       "0          6.562591                         1  \n",
       "1          6.573935                         0  \n",
       "2          6.469369                         0  \n",
       "3          6.457407                         1  \n",
       "4          6.500339                         0  \n",
       "5          6.442997                         0  \n",
       "6          6.369709                         1  \n",
       "7          6.459555                         0  \n",
       "8          6.422143                         0  \n",
       "9          6.314816                         1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing our dataframe \n",
    "\n",
    "stocks_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of our dataframe \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Logistic Regression \n",
    "\n",
    "# Assigning Variables \n",
    "\n",
    "X = stocks_df.drop(['Open', 'Adj Closing Price Change', 'High', 'Low', 'Close'], axis= 1)\n",
    "y = stocks_df['Adj Closing Price Change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test \n",
    "\n",
    "# Import the package \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the test and remainder\n",
    "\n",
    "X_rem, X_test, y_rem, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "\n",
    "# Splitting the train and validation \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rem, y_rem, test_size=0.2, stratify=y_rem, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data \n",
    "\n",
    "# Import the package \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.8395293808334429\n",
      "Validation score 0.8384382805310898\n",
      "Test score 0.8345848056537103\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "#Importing the package \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Intializing the log reg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fitting our training data\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Printing results\n",
    "print(\"Train score\", logreg.score(X_train_scaled, y_train))\n",
    "print(\"Validation score\", logreg.score(X_val_scaled, y_val))\n",
    "print(\"Test score\", logreg.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = logreg.predict(X_test_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  = print my confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.65      0.79     38770\n",
      "           1       0.76      1.00      0.86     42734\n",
      "\n",
      "    accuracy                           0.83     81504\n",
      "   macro avg       0.88      0.83      0.83     81504\n",
      "weighted avg       0.87      0.83      0.83     81504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After down-sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our balanced dataset from the EDA - 2 Notebook \n",
    "\n",
    "balanced_df = pd.read_csv('../sample_balanced_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Closing Yday</th>\n",
       "      <th>Adj Closing Price Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99.500000</td>\n",
       "      <td>100.570000</td>\n",
       "      <td>98.389999</td>\n",
       "      <td>98.580002</td>\n",
       "      <td>89.595734</td>\n",
       "      <td>2450000</td>\n",
       "      <td>90.559128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.160000</td>\n",
       "      <td>47.220001</td>\n",
       "      <td>46.480000</td>\n",
       "      <td>46.970001</td>\n",
       "      <td>44.740360</td>\n",
       "      <td>26747200</td>\n",
       "      <td>45.073746</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>434.312103</td>\n",
       "      <td>436.523804</td>\n",
       "      <td>430.013214</td>\n",
       "      <td>433.270996</td>\n",
       "      <td>433.270996</td>\n",
       "      <td>6056416</td>\n",
       "      <td>438.820190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.070000</td>\n",
       "      <td>24.200001</td>\n",
       "      <td>23.740000</td>\n",
       "      <td>23.930000</td>\n",
       "      <td>18.786953</td>\n",
       "      <td>65512400</td>\n",
       "      <td>18.810499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.049999</td>\n",
       "      <td>58.220001</td>\n",
       "      <td>57.169998</td>\n",
       "      <td>57.860001</td>\n",
       "      <td>53.495506</td>\n",
       "      <td>2572100</td>\n",
       "      <td>53.486259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open        High         Low       Close   Adj Close    Volume  \\\n",
       "0   99.500000  100.570000   98.389999   98.580002   89.595734   2450000   \n",
       "1   47.160000   47.220001   46.480000   46.970001   44.740360  26747200   \n",
       "2  434.312103  436.523804  430.013214  433.270996  433.270996   6056416   \n",
       "3   24.070000   24.200001   23.740000   23.930000   18.786953  65512400   \n",
       "4   58.049999   58.220001   57.169998   57.860001   53.495506   2572100   \n",
       "\n",
       "   Adj Closing Yday  Adj Closing Price Change  \n",
       "0         90.559128                         0  \n",
       "1         45.073746                         0  \n",
       "2        438.820190                         0  \n",
       "3         18.810499                         0  \n",
       "4         53.486259                         1  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5010\n",
       "1    4990\n",
       "Name: Adj Closing Price Change, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df['Adj Closing Price Change'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Logistic Regression \n",
    "\n",
    "# Assigning Variables \n",
    "\n",
    "X_balanced = balanced_df.drop(['Open', 'Adj Closing Price Change', 'High', 'Low', 'Close'], axis= 1)\n",
    "y_balanced = balanced_df['Adj Closing Price Change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test \n",
    "\n",
    "# Import the package \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the test and remainder\n",
    "\n",
    "X_rem, X_test, y_rem, y_test = train_test_split(X_balanced, y_balanced, test_size=0.3, stratify=y_balanced, random_state=1)\n",
    "\n",
    "# Splitting the train and validation \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rem, y_rem, test_size=0.2, stratify=y_rem, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data \n",
    "\n",
    "# Import the package \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.7385714285714285\n",
      "Validation score 0.7428571428571429\n",
      "Test score 0.72\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "#Importing the package \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Intializing the log reg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fitting our training data\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Printing results\n",
    "print(\"Train score\", logreg.score(X_train_scaled, y_train))\n",
    "print(\"Validation score\", logreg.score(X_val_scaled, y_val))\n",
    "print(\"Test score\", logreg.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Class 0</th>\n",
       "      <th>Predicted Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>1456</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>793</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Class 0  Predicted Class 1\n",
       "True Negative               1456                 47\n",
       "True Positive                793                704"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix \n",
    "\n",
    "#Import the confusion matrix libraries\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Generate confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# Label rows and columns\n",
    "cf_df = pd.DataFrame(\n",
    "    cf_matrix,\n",
    "    columns=[\"Predicted Class 0\", \"Predicted Class 1\"],\n",
    "    index=[\"True Negative\", \"True Positive\"]\n",
    ")\n",
    "display(cf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x16b18cf40>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix Display \n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(logreg, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.97      0.78      1503\n",
      "           1       0.94      0.47      0.63      1497\n",
      "\n",
      "    accuracy                           0.72      3000\n",
      "   macro avg       0.79      0.72      0.70      3000\n",
      "weighted avg       0.79      0.72      0.70      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report \n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline to optimize logreg\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "\n",
    "estimators = [\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from tempfile import mkdtemp\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# \n",
    "my_pipe = Pipeline(estimators, memory=cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic Regression\n",
    "\n",
    "logreg_params = {\n",
    "    'scaler': [StandardScaler()],\n",
    "    'model': [LogisticRegression(random_state = 1)],\n",
    "    'model__C': [0.1, 1, 10],\n",
    "    'model__solver': ['liblinear', 'lbfgs'],\n",
    "    'model__penalty': ['l1','l2'],\n",
    "    'model__max_iter': [100000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "logreg_grid = GridSearchCV(my_pipe, param_grid =logreg_params, cv = 5, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 3)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_rem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l1, model__solver=liblinear, scaler=StandardScaler(); total time=   0.1s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l1, model__solver=liblinear, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l1, model__solver=liblinear, scaler=StandardScaler(); total time=   0.2s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l1, model__solver=liblinear, scaler=StandardScaler(); total time=   0.1s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l1, model__solver=liblinear, scaler=StandardScaler(); total time=   0.1s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l1, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l1, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l1, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l1, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l1, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l2, model__solver=liblinear, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l2, model__solver=liblinear, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l2, model__solver=liblinear, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l2, model__solver=liblinear, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l2, model__solver=liblinear, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l2, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l2, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l2, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l2, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=0.1, model__max_iter=100000, model__penalty=l2, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l1, model__solver=liblinear, scaler=StandardScaler(); total time=  23.5s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l1, model__solver=liblinear, scaler=StandardScaler(); total time=  21.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l1, model__solver=liblinear, scaler=StandardScaler(); total time=  18.1s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l1, model__solver=liblinear, scaler=StandardScaler(); total time=  17.6s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l1, model__solver=liblinear, scaler=StandardScaler(); total time=  22.1s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l1, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l1, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l1, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l1, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l1, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l2, model__solver=liblinear, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l2, model__solver=liblinear, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l2, model__solver=liblinear, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l2, model__solver=liblinear, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l2, model__solver=liblinear, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l2, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l2, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l2, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l2, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n",
      "[CV] END model=LogisticRegression(random_state=1), model__C=1, model__max_iter=100000, model__penalty=l2, model__solver=lbfgs, scaler=StandardScaler(); total time=   0.0s\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "logreg_fit_grid = logreg_grid.fit(X_rem, y_rem.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.9841428571428572\n",
      "best parameters: {'model': LogisticRegression(C=10, max_iter=10000, penalty='l1', random_state=1,\n",
      "                   solver='liblinear'), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l1', 'model__solver': 'liblinear', 'scaler': StandardScaler()}\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "print(f\"best score: {logreg_fit_grid.best_score_}\")\n",
    "print(f\"best parameters: {logreg_fit_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
